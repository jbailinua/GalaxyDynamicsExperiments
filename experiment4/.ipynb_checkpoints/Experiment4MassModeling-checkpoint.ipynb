{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Experiment 4: Mass Modeling\n",
    "\n",
    "In this experiment, you will take observed velocity data from some objects and use them to figure out how their mass is distributed.\n",
    "\n",
    "Before you begin, **make sure to read through the full notebook and understand what each function does**. Pay special attention to anything that has a **FIXME** note, which you will need to edit.\n",
    "\n",
    "(Note that we are **not** using the ``%matplotlib notebook`` pragma this time, which should make it easier for you to find your plots!)\n",
    "\n",
    "We're going to be using a slightly complicated combination for units for galpy: we will use units whenever we can, but there's one scipy function and one galpy function that don't work well with physical units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import binned_statistic, linregress\n",
    "from scipy.interpolate import interp1d\n",
    "from astropy import units as u, constants as const\n",
    "import astropy\n",
    "import galpy.util\n",
    "galpy.util.__config__.set('astropy', 'astropy-units', 'True')\n",
    "from galpy import potential, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in SPARC velocity curves and surface brightness profiles\n",
    "\n",
    "We will start off using data from the Spitzer Photometry & Accurate Rotation Curves [SPARC](http://astroweb.cwru.edu/SPARC/) project, which contains rotation curves and Spitzer IRAC 3.6μm surface brightness profiles for 175 late-type galaxies. Light at 3.6μm is a pretty good tracer of stellar mass (much better than in the optical, where it depends much more on the age of the stellar population), with a typical mass-to-light ratio of $\\Upsilon_{[3.6]} \\equiv M/L_{[3.6]} \\approx 0.5 M_{\\odot}/L_{\\odot}$. The rotation curve data come from a variety of sources, using both HI and H$\\alpha$.\n",
    "\n",
    "You should have downloaded a data file that has some properties for all galaxies in the sample (``MaximumDiskData.mrt``), a file containing the actual rotation curves for all galaxies (``MassModels_Lelli2016c.mrt``), a file containing the surface brightness profile of NGC 2998 (``NGC2998.sfb``), and a zip file containing all of the other surface brightness profiles (``sfb_LTG.zip``)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by exploring the galaxy NGC 2998. First let's extract its row from the properties file and find out a little bit about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allgalaxyproperties = Table.read('MaximumDiskData.mrt', format='ascii')\n",
    "N2998_properties = allgalaxyproperties[allgalaxyproperties['ID']=='NGC2998']\n",
    "N2998_properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The properties we will need are the distance (``D``) and inclination (``inc``). The remaining columns mostly contain information about what the SPARC analysis thought about the mass distribution in these galaxies, but we're going to do that ourselves! If you want to pick another galaxy to try later, another useful column might be ``L[3.6]`` which is the total luminosity in units of $10^9~L_{\\odot}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read in the rotation curve file, and extract NGC 2998's rotation curve, and look at the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotcurves_allgalaxies = Table.read('MassModels_Lelli2016c.mrt', format='ascii.cds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotcurve_N2998 = rotcurves_allgalaxies[rotcurves_allgalaxies['ID']=='NGC2998']\n",
    "rotcurve_N2998"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cylindrical radius is in the ``R`` column, and the observed rotation values are in the ``Vobs`` column, with errors in ``e_Vobs``. The other columns indicate the mass decomposition from SPARC, which again we're going to do ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to read in is the surface brightness profiles. They are in magnitudes per square arcsecond, called ``mu``. All surface brightness units are horrendous, but that one is even more horrendous than most. The conversion between $\\mu$ and a more physical unit like $L_{\\odot}/\\mathrm{pc^2}$ is given by\n",
    "\n",
    "$$ \\mu = M_{\\mathrm{abs},\\odot} - 2.5 \\log \\frac{I \\theta^2 (10 \\mathrm{pc})^2}{L_{\\odot}} $$\n",
    "where $I$ is in units of $L_{\\odot}/\\mathrm{pc^2}$, $M_{\\mathrm{abs},\\odot}$ is the absolute magnitude of the Sun in the observed band ($M_{\\mathrm{abs},\\odot} = 3.24$ for the IRAC 3.6μm band), $\\theta$ is the conversion between arcseconds and radians, and logarithms are always base 10 unless stated otherwise. Here is a function that will convert back and forth as necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surfbright(I=None, mu=None, solarabsmag=3.24):\n",
    "    \"\"\"Converts surface brightness between physical intensity I in units of energy per time per area\n",
    "    (e.g. Lsun / pc^2) and in magnitudes per square arcsecond. Note that mu should be given (or will\n",
    "    be returned) as a unitless number, and I should be a quantity with appropriate dimension.\n",
    "    \n",
    "    Assumes Spitzer IRAC 3.6 microns, where absolute magnitude of the Sun is 3.24. For a different band, set\n",
    "    solarabsmag.\"\"\"\n",
    "    if np.sum([I is None, mu is None]) != 1:\n",
    "        raise ValueError(\"Exactly one of I and mu must be specified.\")\n",
    "\n",
    "    absmagdist = 10. * u.pc\n",
    "    absmagdist2 = absmagdist**2\n",
    "    arcsec_rad2 = ((1 * u.arcsec).to(u.rad).value)**2\n",
    "\n",
    "    if I is None:\n",
    "        # mu is given\n",
    "        brightness_per_area = (1 * u.Lsun / (arcsec_rad2 * absmagdist2)) * 10.**(-0.4*(mu - solarabsmag))\n",
    "        return {'mu':mu, 'I':brightness_per_area}\n",
    "        \n",
    "    else:\n",
    "        # I is given\n",
    "        logarg = (I * arcsec_rad2 * absmagdist2) / u.Lsun\n",
    "        mag_per_arcsec2 = solarabsmag - 2.5 * np.log10(logarg.to(1).value)\n",
    "        return {'mu':mag_per_arcsec2, 'I':I}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the surface brightness data and convert it to physical units. Note that the SPARC profiles are **not** corrected for inclination, so we need to multiply by $\\cos(i)$ to get the deprojected surface brightness (that is good enough as long as it's not really close to edge-on -- anything with $i<80^o$ should work fine). Also, the data has radius in arcseconds, but we would like it in kpc. Finally, it is useful to convert the error in magnitudes to an error in absolute units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbdat_N2998 = Table.read('NGC2998.sfb', format='ascii')\n",
    "surfbright_N2998 = surfbright(mu=sbdat_N2998['mu'])['I'] * np.cos(N2998_properties['inc']*u.deg)\n",
    "radiuskpc_N2998 = (sbdat_N2998['radius']*u.arcsec/u.rad).to(1) * N2998_properties['D']*1e3*u.kpc\n",
    "surfbright_err_N2998 = np.abs(surfbright_N2998 * (10.**(-0.4*sbdat_N2998['error'])-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great -- let's plot them up and see what we have!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation curve\n",
    "plt.errorbar(rotcurve_N2998['R'], rotcurve_N2998['Vobs'], yerr=rotcurve_N2998['e_Vobs'], label='NGC 2998')\n",
    "plt.xlabel('R ({0})'.format(str(rotcurve_N2998['R'].unit)))\n",
    "plt.ylabel('$v_c$ ({0})'.format(str(rotcurve_N2998['Vobs'].unit)))\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surface brightness profile\n",
    "plt.errorbar(radiuskpc_N2998.value, surfbright_N2998.value, yerr=surfbright_err_N2998.value, label='NGC 2998')\n",
    "plt.xlabel('R ({0})'.format(str(radiuskpc_N2998.unit)))\n",
    "plt.ylabel('I ({0})'.format(str(surfbright_N2998.unit)))\n",
    "plt.yscale('log')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at them! What do you think about this galaxy? How would you describe its rotation curve? How would you describe its surface brightness profile?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What rotation curve would you expect from the disk?\n",
    "\n",
    "Given the properties of the observed disk, we can predict what the rotation curve should look like, and compare it to the observed one.\n",
    "\n",
    "First, it would be useful to have a measurement for the total mass of the galaxy. We will look at the enclosed mass within two radii: the radius of the last point on the rotation curve, and also at 100 kpc.\n",
    "\n",
    "**FIXME:** Given the circular velocity at the final point, you should be able to calculate the mass enclosed, as we discussed in Assignment 2. Note that if you index a ``numpy`` array with a negative number, it counts back from the end of the array, so ``foo[-1]`` gives the last element of ``foo`` (note, however, that in this case it will strip the units off, so we need to add them back on). Also, make sure to take advantage of the astropy ``constants`` module (imported above as ``const`` so you can use things like ``const.G``), and the ``units`` module... so, for example, to convert something that has dimensions of mass but perhaps odd-looking units to solar masses, you can use:\n",
    "\n",
    "``foo = (...some.weird.expression...).to(u.Msun)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius_of_final_point = rotcurve_N2998['R'][-1] * u.kpc\n",
    "vc_at_final_point = rotcurve_N2998['Vobs'][-1] * u.km/u.s\n",
    "enclosed_mass_within_final_point = #FIXME\n",
    "print('From final point in rotation curve (vc=',vc_at_final_point,'):')\n",
    "print(' Mass enclosed within',radius_of_final_point,' is {0:e}.'.format(enclosed_mass_within_final_point))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you know about the mass enclosed within 100 kpc?\n",
    "\n",
    "Now let's create a galpy potential object for the disk. We will assume that it is exponential ($\\Sigma(R) = \\Sigma_0 e^{-R/Rd}$). Specifically, we will fit the surface brightness profile to the following form:\n",
    "\n",
    "$$ I(R) = I_0 e^{-R/Rd} $$\n",
    "\n",
    "and then turn that into a surface density $\\Sigma$ by assuming a constant $M/L$.\n",
    "\n",
    "To do the fitting, we will use ``scipy.optimize.curve_fit``. This is not the only -- or best, for that matter -- way of fitting functions, but it works well enough for this problem.\n",
    "\n",
    "To use ``curve_fit``, we need to define the function that we want to fit. That function must take an argument that is the independent variable, and as other arguments any parameters that we want to fit. In this case, the independent variable is the radius, and the parameters are the amplitude $I_0$ and the scale radius $Rd$.\n",
    "\n",
    "**FIXME:** Define the function that returns the value of $I(R)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that defines an exponential\n",
    "def exponential_fit(r, amplitude, scale):\n",
    "    return #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we feed the radii and surface brightnesses into ``curve_fit``, along with the name of the function we defined. Note that ``scipy`` functions don't know about ``astropy`` units and often have trouble with them -- it is easiest when using ``curve_fit`` to enforce a certain unit system and strip off the units using ``.value``. In this case, we will put all distances in kpc, all velocities in km/s, and all surface brightnesses in $L_{\\odot}/\\mathrm{pc^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be wary here: it turns out that fitting curves that depend on multiple parameters is one of those computational problems that requires more thought than you might have expected. To understand why, you need to know more about how the algorithm works.\n",
    "\n",
    "Essentially, the algorithm starts off with an initial guess of what the best parameters are, and it evaluates the function using those parameters to see how closely they match the data. It then varies the parameters a bit, evaluates the function on each of those variations, and sees which direction the parameters need to change to get a better fit. It then moves along in that direction in parameter space, and tries some more sets of parameters, until it finally reaches a point where no matter how it changes the parameters, the match between the function and the data gets worse. At that point, it has minimized the difference between the function predictions and the data, and so this is called a \"minimum\".\n",
    "\n",
    "However, these algorithms are sensitive to the starting point, and can get trapped in \"local minima\". For example, if this image shows the difference between the function and the data depending on the value of 2 parameters, the true best fit is at the point labeled \"Global Minima\" -- that's where the difference is the smallest. But if it starts at the point labeled A, then best \"best way forward\" is into the nearby \"Local Minima\" -- the fit is better than at A. However, once it gets there, it can only get to the global minimum by going through values where the fit is *worse*.\n",
    "\n",
    "![Local Minima](https://miro.medium.com/max/1400/1*ZC9qItK9wI0F6BwSVYMQGg.png)\n",
    "\n",
    "There are a variety of algorithms that have been created over the years to attempt to avoid this problem. However, the simplest way of avoiding it is to make sure that the starting point is not that far from what the final solution will be. In our case, ``curve_fit``'s default starting point is that every parameter has a value of 1. In some cases that might be fine... but in some cases it won't be. So we should always give it a hint about where to start.\n",
    "\n",
    "We know that the scale radius should be somewhere within the radii of the data, and the amplitude should be somewhere near the value at the center, so we can use the central value and the median radius as initial guesses using the ``p0`` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenunits = u.kpc\n",
    "sbunits = u.Lsun / u.pc**2\n",
    "velunits = u.km/u.s\n",
    "\n",
    "initial_guess = [surfbright_N2998[0].to(sbunits).value, np.median(radiuskpc_N2998.to(lenunits).value)]\n",
    "\n",
    "exponential_fit_parms, exponential_fit_pcov = curve_fit(exponential_fit, radiuskpc_N2998.to(lenunits).value, \\\n",
    "                                    surfbright_N2998.to(sbunits).value, sigma=surfbright_err_N2998.to(sbunits).value)\n",
    "exponential_fit_parms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first return from ``curve_fit`` is the best fit value of all of the parameters, in order, and the second return is the covariance matrix. What are the best fit values? Do they make sense?\n",
    "\n",
    "Let's try plotting the best fit onto the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surface brightness profile\n",
    "# Plot the fit from 0 to 10% larger than the last data point\n",
    "rax = np.linspace(0, np.max(radiuskpc_N2998.value)*1.1, 100)*u.kpc\n",
    "plt.errorbar(radiuskpc_N2998.value, surfbright_N2998.value, yerr=surfbright_err_N2998.value, label='NGC 2998')\n",
    "plt.plot(rax, exponential_fit(rax.value, exponential_fit_parms[0], exponential_fit_parms[1]), \\\n",
    "        label='Best fit')\n",
    "plt.xlabel('R ({0})'.format(str(radiuskpc_N2998.unit)))\n",
    "plt.ylabel('I ({0})'.format(str(surfbright_N2998.unit)))\n",
    "plt.yscale('log')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of the fit? Is it good enough? What might be going on?\n",
    "\n",
    "It might help to change the plot to a linear scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rax = np.linspace(0, np.max(radiuskpc_N2998.value)*1.1, 100)*u.kpc\n",
    "plt.errorbar(radiuskpc_N2998.value, surfbright_N2998.value, yerr=surfbright_err_N2998.value, label='NGC 2998')\n",
    "plt.plot(rax, exponential_fit(rax.value, exponential_fit_parms[0], exponential_fit_parms[1]), \\\n",
    "        label='Best fit')\n",
    "plt.xlabel('R ({0})'.format(str(radiuskpc_N2998.unit)))\n",
    "plt.ylabel('I ({0})'.format(str(surfbright_N2998.unit)))\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Why did it come up with that fit?\n",
    "\n",
    "When fitting data that varies by many orders of magnitude and you want the fit to pay attention to both large and small values, you can perform the fit *in log space*. In other words, have the function return the logarithm of the value, and have it fit the logarithm of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that defines an exponential in log space\n",
    "# This should just be the logarithm of the exponential_fit function from earlier.\n",
    "# Use natural log because it's slightly faster to compute and we never see the value.\n",
    "def exponential_in_logspace_fit(r, amplitude, scale):\n",
    "    return np.log(exponential_fit(r, amplitude, scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to transform the errors into log space too.\n",
    "sberr_logspace = np.abs(-0.4*surfbright_err_N2998.to(sbunits).value*np.log(10))\n",
    "\n",
    "# We need to give the logarithm of the surface brightness values to curve_fit\n",
    "explogspace_fit_parms, explogspace_fit_pcov = curve_fit(exponential_in_logspace_fit,\\\n",
    "                radiuskpc_N2998.to(lenunits).value, np.log(surfbright_N2998.to(sbunits).value), \\\n",
    "                sigma=sberr_logspace, p0=initial_guess)\n",
    "\n",
    "explogspace_fit_parms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare these to the earlier fits. Do the parameters look plausible? Is it obvious from the numbers whether this is better?\n",
    "\n",
    "Let's plot both fits up, both in linear and log space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "ax1.errorbar(radiuskpc_N2998.value, surfbright_N2998.value, yerr=surfbright_err_N2998.value, label='NGC 2998')\n",
    "ax1.plot(rax, exponential_fit(rax.value, exponential_fit_parms[0], exponential_fit_parms[1]), label='Exponential (Linear Fit)')\n",
    "ax1.plot(rax, exponential_fit(rax.value, explogspace_fit_parms[0], explogspace_fit_parms[1]), label='Exponential (Log Space Fit)')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.errorbar(radiuskpc_N2998.value, surfbright_N2998.value, yerr=surfbright_err_N2998.value, label='NGC 2998')\n",
    "ax2.plot(rax, exponential_fit(rax.value, exponential_fit_parms[0], exponential_fit_parms[1]), label='Exponential (Linear Fit)')\n",
    "ax2.plot(rax, exponential_fit(rax.value, explogspace_fit_parms[0], explogspace_fit_parms[1]), label='Exponential (Log Space Fit)')\n",
    "ax2.set_ylim(1e-2,1e4)\n",
    "ax2.set_yscale('log')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of these two fits?\n",
    "\n",
    "Let's use the log space fit, which seems to provide a better description of most of the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I0_N2998 = explogspace_fit_parms[0] * u.Lsun / u.pc**2\n",
    "Rd_N2998 = explogspace_fit_parms[1] * u.kpc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a potential, we need mass surface density, and then we can use ``potential.RazorThinExponentialDiskPotential``. For now we will assume $\\Upsilon_{[3.6]} = 0.5 M_{\\odot}/L_{\\odot}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML = 0.5 * u.Msun/u.Lsun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expdisk_potential = potential.RazorThinExponentialDiskPotential(amp=I0_N2998*ML, hr=Rd_N2998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can plot the rotation curve we would expect if the visible disk were the only source of gravity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a radius range that goes 10% farther than the last data point\n",
    "rax = np.linspace(0, np.max(rotcurve_N2998['R'].value)*1.10, 200) * u.kpc\n",
    "plt.errorbar(rotcurve_N2998['R'], rotcurve_N2998['Vobs'], yerr=rotcurve_N2998['e_Vobs'], label='NGC 2998')\n",
    "plt.plot(rax, potential.vcirc(expdisk_potential, rax), label='Disk (M/L=0.5)')\n",
    "plt.xlabel('R ({0})'.format(str(rotcurve_N2998['R'].unit)))\n",
    "plt.ylabel('$v_c$ ({0})'.format(str(rotcurve_N2998['Vobs'].unit)))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at this! What do you think? Is the disk rotation curve below the observed $v_c$ at any radii? If so, what does that mean? Does the disk rotation curve match the observed $v_c$ at any radii? If so, what does that mean? Is the disk rotation curve above the observed $v_c$ at any radii? If so, what does that mean? (Even if any of these are not true about this particular comparison, when I say \"How do the predicted and observed rotation curves compare?\" in the rest of this experiment, I want you to think about all of these questions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check what the enclosed mass is for this mass model, at the same radii as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('From disk-only model (M/L=0.5):')\n",
    "print(' Mass enclosed within',radius_of_final_point,' is {0:e}'.format(potential.mass(expdisk_potential, radius_of_final_point)))\n",
    "print(' Mass enclosed within 100 kpc is {0:e}.'.format(potential.mass(expdisk_potential, 100*u.kpc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this compare to your estimate from the end of the rotation curve? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add an NFW halo\n",
    "\n",
    "Let's add in an NFW halo and see if we can get a better fit to the rotation curve. We will define a few functions: one of which returns a potential that contains both the disk and NFW for a given set of parameters, and one that returns the circular velocity with the units stripped out so that ``curve_fit`` can deal with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a potential object that contains the disk + NFW halo\n",
    "# Note that we're allowing two optional parameters: disk_ML that has a default value of 0.5,\n",
    "# and disk_rs that has a default value of Rd_N2998.\n",
    "# We won't use them right now, but it will be helpful below when we want to be able to change them.\n",
    "# Return each component separately, plus the total.\n",
    "def disk_plus_NFW_potential(nfw_amp, nfw_rs, disk_ML=0.5, disk_rs=None):\n",
    "    if disk_rs is None:\n",
    "        _disk_rs = Rd_N2998\n",
    "    else:\n",
    "        _disk_rs = disk_rs * u.kpc\n",
    "        \n",
    "    disk_potential = potential.RazorThinExponentialDiskPotential(amp=I0_N2998*disk_ML*u.Msun/u.Lsun, hr=_disk_rs)\n",
    "    halo_potential = potential.NFWPotential(amp=nfw_amp*u.Msun, a=nfw_rs*u.kpc)\n",
    "    total_potential = disk_potential + halo_potential\n",
    "    return {'Disk M/L={0:.2f}'.format(disk_ML):disk_potential, 'NFW':halo_potential, 'Total':total_potential}\n",
    "\n",
    "# Return the circular velocity of the disk + NFW halo potential, in units of km/s but with the units removed.\n",
    "# Note that the input parameters also need to be unitless -- we are going to use units of solar masses for\n",
    "# the NFW amplitude, kpc for the NFW scale radius and disk scale radius, and Msun/Lsun for the disk M/L.\n",
    "# Also make the input radius in kpc.\n",
    "def disk_plus_NFW_vc(r, nfw_amp, nfw_rs, disk_ML=0.5, disk_rs=None):\n",
    "    total_potential = disk_plus_NFW_potential(nfw_amp, nfw_rs, disk_ML, disk_rs)['Total']\n",
    "    return potential.vcirc(total_potential, r*u.kpc).to(u.km/u.s).value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit for the best value of the NFW halo.\n",
    "\n",
    "**FIXME:** Remember to give a good initial guess for the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess_nfw_amp = #FIXME\n",
    "initial_guess_nfw_rs = #FIXME\n",
    "initial_guess = [initial_guess_nfw_amp, initial_guess_nfw_rs]\n",
    "\n",
    "disk_ML05_plus_NFW_fit_parms, disk_ML05_plus_NFW_fit_pcov = curve_fit(disk_plus_NFW_vc, \\\n",
    "    rotcurve_N2998['R'].value, rotcurve_N2998['Vobs'].value, \\\n",
    "    sigma=rotcurve_N2998['e_Vobs'].value, p0=initial_guess)\n",
    "\n",
    "disk_ML05_plus_NFW_fit_parms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the best fit amplitude and scale radius. Do they seem reasonable?\n",
    "\n",
    "Now let's plot the best fit rotation curve, along with each individual component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rax = np.linspace(0, np.max(rotcurve_N2998['R'].value)*1.10, 200) * u.kpc\n",
    "plt.errorbar(rotcurve_N2998['R'], rotcurve_N2998['Vobs'], yerr=rotcurve_N2998['e_Vobs'], label='NGC 2998')\n",
    "\n",
    "# Plot each piece of the potential. Note that putting the * in front of the argument\n",
    "# expands it out in the function call. For example, if foo=[10, 20] then\n",
    "#   myfunc(*foo)\n",
    "# is equivalent to\n",
    "#   myfunc(10, 20)\n",
    "for potname, potobj in disk_plus_NFW_potential(*disk_ML05_plus_NFW_fit_parms).items():\n",
    "    plt.plot(rax, potential.vcirc(potobj, rax), label=potname)\n",
    "\n",
    "plt.xlabel('R ({0})'.format(str(rotcurve_N2998['R'].unit)))\n",
    "plt.ylabel('$v_c$ ({0})'.format(str(rotcurve_N2998['Vobs'].unit)))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the predicted and observed rotation curves compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Allow the disk parameters to vary\n",
    "\n",
    "Although a typical stellar population has $\\Upsilon_{[3.6]} \\approx 0.5$, it will depend somewhat on the stellar population (e.g. how much very recent star formation it has experienced). So the fit might be improved if we allow that to vary. Fortunately, given the way we wrote the functions above, we just need to include an initial guess for that parameter and it will become part of the fit.\n",
    "\n",
    "**FIXME:** Choose an appropriate initial guess for M/L."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the initial guesses for the NFW parameters were okay, above, they should be fine here too!\n",
    "initial_guess_ML = #FIXME\n",
    "initial_guess = [initial_guess_nfw_amp, initial_guess_nfw_rs, initial_guess_ML]\n",
    "\n",
    "disk_noML_plus_NFW_fit_parms, disk_noML_plus_NFW_fit_pcov = curve_fit(disk_plus_NFW_vc, \\\n",
    "    rotcurve_N2998['R'].value, rotcurve_N2998['Vobs'].value, \\\n",
    "    sigma=rotcurve_N2998['e_Vobs'].value, p0=initial_guess)\n",
    "\n",
    "disk_noML_plus_NFW_fit_parms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the values of the parameters (NFW amplitude, NFW scale radius, and disk M/L). Do they seem reasonable?\n",
    "\n",
    "Let's plot the best fit rotation curve, along with each individual component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rax = np.linspace(0, np.max(rotcurve_N2998['R'].value)*1.10, 200) * u.kpc\n",
    "plt.errorbar(rotcurve_N2998['R'], rotcurve_N2998['Vobs'], yerr=rotcurve_N2998['e_Vobs'], label='NGC 2998')\n",
    "\n",
    "for potname, potobj in disk_plus_NFW_potential(*disk_noML_plus_NFW_fit_parms).items():\n",
    "    plt.plot(rax, potential.vcirc(potobj, rax), label=potname)\n",
    "\n",
    "plt.xlabel('R ({0})'.format(str(rotcurve_N2998['R'].unit)))\n",
    "plt.ylabel('$v_c$ ({0})'.format(str(rotcurve_N2998['Vobs'].unit)))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about what's going on here?\n",
    "\n",
    "We can force the fit to avoid parameter values that are unphysical by making the output function give absurd results when it gets those. Formally, we apply a \"penalty\" -- add or subtract some very large number so the fit gets extremely bad and the fitting function learns to stay away from those values.\n",
    "\n",
    "**FIXME:** Rewrite the ``disk_plus_NFW_vc`` function to apply a penalty somewhere that you think is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disk_plus_NFW_vc(r, nfw_amp, nfw_rs, disk_ML=0.5, disk_rs=None):\n",
    "    total_potential = disk_plus_NFW_potential(nfw_amp, nfw_rs, disk_ML, disk_rs)['Total']\n",
    "    \n",
    "    if #FIXME: Put your condition here:\n",
    "        penalty = -1e6    # subtract one million km/s\n",
    "    else:\n",
    "        penalty = 0.\n",
    "    \n",
    "    return potential.vcirc(total_potential, r*u.kpc).to(u.km/u.s).value + penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now perform the fit again and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disk_noML_plus_NFW_fit_parms, disk_noML_plus_NFW_fit_pcov = curve_fit(disk_plus_NFW_vc, \\\n",
    "    rotcurve_N2998['R'].value, rotcurve_N2998['Vobs'].value, \\\n",
    "    sigma=rotcurve_N2998['e_Vobs'].value, p0=initial_guess)\n",
    "\n",
    "disk_noML_plus_NFW_fit_parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rax = np.linspace(0, np.max(rotcurve_N2998['R'].value)*1.10, 200) * u.kpc\n",
    "plt.errorbar(rotcurve_N2998['R'], rotcurve_N2998['Vobs'], yerr=rotcurve_N2998['e_Vobs'], label='NGC 2998')\n",
    "\n",
    "for potname, potobj in disk_plus_NFW_potential(*disk_noML_plus_NFW_fit_parms).items():\n",
    "    plt.plot(rax, potential.vcirc(potobj, rax), label=potname)\n",
    "\n",
    "plt.xlabel('R ({0})'.format(str(rotcurve_N2998['R'].unit)))\n",
    "plt.ylabel('$v_c$ ({0})'.format(str(rotcurve_N2998['Vobs'].unit)))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of the parameter values? What do you think of the fit?\n",
    "\n",
    "Let's try allowing the disk scale length to vary too. Consider why we are doing this from a fitting-the-data standpoint, and also to what degree this makes physical sense.\n",
    "\n",
    "As with adding the disk M/L, the functions are already set up to allow this as long as we give an initial guess. In this case, that should obviously be the best fit scale length from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess_diskrs = Rd_N2998.to(u.kpc).value\n",
    "initial_guess = [initial_guess_nfw_amp, initial_guess_nfw_rs, initial_guess_ML, initial_guess_diskrs]\n",
    "\n",
    "disk_rs_plus_NFW_fit_parms, disk_rs_plus_NFW_fit_pcov = curve_fit(disk_plus_NFW_vc, \\\n",
    "    rotcurve_N2998['R'].value, rotcurve_N2998['Vobs'].value, \\\n",
    "    sigma=rotcurve_N2998['e_Vobs'].value, p0=initial_guess)\n",
    "\n",
    "disk_rs_plus_NFW_fit_parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rax = np.linspace(0, np.max(rotcurve_N2998['R'].value)*1.10, 200) * u.kpc\n",
    "plt.errorbar(rotcurve_N2998['R'], rotcurve_N2998['Vobs'], yerr=rotcurve_N2998['e_Vobs'], label='NGC 2998')\n",
    "\n",
    "for potname, potobj in disk_plus_NFW_potential(*disk_rs_plus_NFW_fit_parms).items():\n",
    "    plt.plot(rax, potential.vcirc(potobj, rax), label=potname)\n",
    "\n",
    "plt.xlabel('R ({0})'.format(str(rotcurve_N2998['R'].unit)))\n",
    "plt.ylabel('$v_c$ ({0})'.format(str(rotcurve_N2998['Vobs'].unit)))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again... what do you think about the best fit values? Think both about the rotation curve and the surface brightness profile.\n",
    "\n",
    "The point I made way back when about falling into local minima gets more and more important the more parameters we fit. Now that we're up to 4 parameters, this is something we should really worry about! Let's try using a different initial guess -- what if we use the disk parameters from the linear fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I0_N2998 = exponential_fit_parms[0] * u.Lsun / u.pc**2\n",
    "Rd_N2998 = exponential_fit_parms[1] * u.kpc\n",
    "\n",
    "initial_guess_diskrs = Rd_N2998.to(u.kpc).value\n",
    "initial_guess = [initial_guess_nfw_amp, initial_guess_nfw_rs, initial_guess_ML, initial_guess_diskrs]\n",
    "\n",
    "alt_disk_rs_plus_NFW_fit_parms, alt_disk_rs_plus_NFW_fit_pcov = curve_fit(disk_plus_NFW_vc, \\\n",
    "    rotcurve_N2998['R'].value, rotcurve_N2998['Vobs'].value, \\\n",
    "    sigma=rotcurve_N2998['e_Vobs'].value, p0=initial_guess)\n",
    "\n",
    "alt_disk_rs_plus_NFW_fit_parms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these the same? Let's plot the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rax = np.linspace(0, np.max(rotcurve_N2998['R'].value)*1.10, 200) * u.kpc\n",
    "plt.errorbar(rotcurve_N2998['R'], rotcurve_N2998['Vobs'], yerr=rotcurve_N2998['e_Vobs'], label='NGC 2998')\n",
    "\n",
    "for potname, potobj in disk_plus_NFW_potential(*alt_disk_rs_plus_NFW_fit_parms).items():\n",
    "    plt.plot(rax, potential.vcirc(potobj, rax), label=potname)\n",
    "\n",
    "plt.xlabel('R ({0})'.format(str(rotcurve_N2998['R'].unit)))\n",
    "plt.ylabel('$v_c$ ({0})'.format(str(rotcurve_N2998['Vobs'].unit)))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about this fit? Which surface brightness fit is more accurate where most of the mass is? Which one gives the more sensible rotation curve decomposition? What might you do to create a model that is sensible over a large range of radius?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check the enclosed mass implied by each of these mass models. **FIXME:** Complete this section following the same idea as the first case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disk_ML05_plus_NFW_potential = disk_plus_NFW_potential(*disk_ML05_plus_NFW_fit_parms)['Total']\n",
    "print('From M/L=0.5 disk plus fit NFW model:')\n",
    "print(' Mass enclosed within',radius_of_final_point,' is {0:e}.'.format(potential.mass(disk_ML05_plus_NFW_potential, radius_of_final_point)))\n",
    "print(' Mass enclosed within 100 kpc is {0:e}'.format(potential.mass(disk_ML05_plus_NFW_potential, 100*u.kpc)))\n",
    "\n",
    "# FIXME: Do the same for all of the other fits you've explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at these and compare them to your constraint from just the rotation curve, along with the disk-only value. Which fits do think are more accurate? What have you learned about the mass distribution of this galaxy? How well constrained is the mass enclosed at different radii?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Do something interesting!\n",
    "\n",
    "First, think about the consequences of this exercise for pipelines that take in large amounts of data (e.g. LSST) and do automated analysis. This is a really great topic for discussion!\n",
    "\n",
    "Then **FIXME:**\n",
    " try something interesting! Here are a few ideas (not mutually exclusive -- you may want to combine them):\n",
    " - Fit a more complicated model to the surface brightness profile (e.g. the sum of two disks) and see how that affects the mass models.\n",
    " - Use a different fitting routine for the rotation curves. For example, MCMC is a very popular algorithm; in astronomy, Dan Foreman-Mackey's [emcee](https://emcee.readthedocs.io/en/stable/) package is the most common one.\n",
    "     * If you do this, it's very easy to explore the correlations between the fit parameters using a corner plot.\n",
    "     * Also, you can try calculating the total enclosed mass for each member of the ensemble. How much uncertainty is there in each individual parameter vs. in the enclosed mass?\n",
    " - Try fitting a different galaxy that has a different-looking rotation curve.\n",
    "     * You will need to unzip the ``sfb_LTG.zip`` file to get the surface brightness profiles of the other galaxies. In Linux and Mac, you can extract a single file from it at the command line using ``unzip sfb_LTG.zip GALAXYNAME.sfb``.\n",
    "     * If you do this with a galaxy that has a prominent bulge, try fitting a bulge+disk to the surface brightness profile, and make a mass model that uses both the bulge and the disk.\n",
    " - Try using different models for the halo instead of NFW. Do any of them do a noticeably better or worse job? How do the total masses compare?\n",
    " - Explore the covariance matrix -- what is the uncertainty in each parameter? How correlated are they?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Read in Milky Way globular cluster kinematic data\n",
    "\n",
    "Now we are going to use the Jeans equation and the velocities of the Milky Way's globular clusters to measure the mass profile of the Milky Way. Remember that the time-independent spherically-symmetric Jeans equation is:\n",
    "\n",
    "$$ \\frac{1}{\\nu} \\frac{d}{dr} \\left(\\nu \\overline{v_r^2}\\right) + 2 \\frac{\\beta}{r} \\overline{v_r^2} = -\\frac{d\\Phi}{dr} = - \\frac{G M(<r)}{r} $$\n",
    "\n",
    "Remembering that $\\overline{v_r^2} = \\bar{v}_r^2 + \\sigma_r^2$, and that this is the time-independent equation, what is $\\overline{v_r^2}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our data source, we will use Gaia positions and 3D velocities as catalogued by [Vasiliev & Baumgardt (2021)](https://ui.adsabs.harvard.edu/abs/2021MNRAS.505.5978V/abstract), contained in the included file ``VB21.txt``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCdat = Table.read('VB21.txt', format='ascii.tab', data_start=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCdat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most interesting columns for us are the 3D Galactocentric Cartesian coordinates $X$, $Y$, and $Z$, and their respective velocities $U$, $V$, and $W$. The $X$ direction is usually defined to be the direction from the Galactic Center towards the Sun, the $Y$ direction is in the direction of the Sun's rotation, and the $Z$ direction is the vertical direction towards the North Galactic Pole (this is, somewhat confusingly, a left handed coordinate system, so you will sometimes see $X$ defined in the opposite direction instead). The Galactocentric radius $R_{\\mathrm{GC}}$ is also useful.\n",
    "\n",
    "We need to transform the Cartesian velocities into spherical coordinate components:\n",
    "$$ v_r = \\hat{r} \\cdot \\vec{v} = \\frac{x v_x + y v_y + z v_z}{r} $$\n",
    "$$ v_{\\theta} = \\hat{\\theta} \\cdot \\vec{v} = \\frac{z}{R r} (x v_x + y v_y) - \\frac{v_z R}{r} $$\n",
    "$$ v_{\\phi} = \\hat{\\phi} \\cdot \\vec{v} = \\frac{x v_y - y v_x}{R} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that for extremely radial orbits, the slight difference in precision between the tabulated\n",
    "# R_GC and the calculated sqrt(X^2 + Y^2 + Z^2) can give a nonsense value for the tangential velocities,\n",
    "# so we manually re-calculate it ourself. Same for cylindrical radius.\n",
    "GCdat['R_GC'] = np.sqrt(GCdat['X']**2 + GCdat['Y']**2 + GCdat['Z']**2)\n",
    "GCdat['Rcyl_GC'] = np.sqrt(GCdat['X']**2 + GCdat['Y']**2)\n",
    "GCdat['Vrad'] = (GCdat['X']*GCdat['U'] + GCdat['Y']*GCdat['V'] + GCdat['Z']*GCdat['W']) / GCdat['R_GC'] * (u.km/u.s)\n",
    "GCdat['Vtheta'] = ( (GCdat['Z']*(GCdat['U']*GCdat['X'] + GCdat['V']*GCdat['Y'])/(GCdat['Rcyl_GC']*GCdat['R_GC'])) - \\\n",
    "                  GCdat['W']*GCdat['Rcyl_GC']/GCdat['R_GC'] ) * (u.km/u.s)\n",
    "GCdat['Vphi'] = (GCdat['X']*GCdat['V'] - GCdat['Y']*GCdat['U'])/GCdat['Rcyl_GC'] * (u.km/u.s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,3))\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "ax1.scatter(GCdat['R_GC'], GCdat['Vrad'], s=2)\n",
    "ax1.set_xlabel('R (kpc)')\n",
    "ax1.set_ylabel('$v_r$ (km/s)')\n",
    "\n",
    "ax2.scatter(GCdat['R_GC'], GCdat['Vtheta'], s=2)\n",
    "ax2.set_xlabel('R (kpc)')\n",
    "ax2.set_ylabel('$v_{\\\\theta}$ (km/s)')\n",
    "\n",
    "ax3.scatter(GCdat['R_GC'], GCdat['Vphi'], s=2)\n",
    "ax3.set_xlabel('R (kpc)')\n",
    "ax3.set_ylabel('$v_{\\phi}$ (km/s)')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at them. How would you describe the radial and tangential velocities of the Milky Way's globular clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Measure the velocity dispersions in radial bins\n",
    "\n",
    "In order to measure the dispersions as a function of radius, we need to split them up into radial bins. But most clusters are at smaller radius, and we need each bin to have a sufficient number of data points to measure its standard deviation. We will use logarithmically-spaced bin edges, with a total of 10 bins so there are approximately 10-15 data points per bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go from just inside the innermost data point to slightly outside the outermost data point\n",
    "innermost_radius = np.min(GCdat['R_GC']) - 0.02\n",
    "outermost_radius = np.max(GCdat['R_GC']) + 1.0\n",
    "# 10 bins spaced logarithmically\n",
    "binedges = np.logspace(np.log10(innermost_radius), np.log10(outermost_radius), 10)\n",
    "\n",
    "# Store the midpoint of every bin -- this is the radius that we plot the bin properties at\n",
    "midrad = 0.5*(binedges[1:] + binedges[:-1]) * u.kpc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate a function for each bin of data, we can use the handy ``scipy.stats.binned_statistic`` function. We feed it the $x$ coordinates that we want to use for binning (e.g. radius in our case), the $y$ value that we want to measure a statistic of (e.g. velocity), and what statistic we want (some useful ones are ``meean``, ``std``, and ``count``; you can also give it your own function). The binned values are in the return value's ``.statistic`` field (note that ``binned_statistic`` strips out the units, so we'll need to add them back)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First count how many GCs are in each bin. Here there is no y value -- it's just a histogram.\n",
    "N_GC = binned_statistic(GCdat['R_GC'], None, bins=binedges, statistic='count').statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot N_GC versus radius\n",
    "plt.plot(midrad, N_GC)\n",
    "plt.xlabel('R (kpc)')\n",
    "plt.ylabel('N')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we succeed in getting 10-15 GCs per bin? Did we at least get enough to calculate a dispersion in every bin?\n",
    "\n",
    "Now calculate the radial velocity dispersion $\\sigma_r$ and its uncertainty. Note that Poisson statistics give an error on the standard deviation of $\\sigma / \\sqrt{2N}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmar_obs = binned_statistic(GCdat['R_GC'], GCdat['Vrad'], bins=binedges, statistic='std').statistic * u.km/u.s\n",
    "sigmar_err_obs = sigmar_obs / np.sqrt(2. * N_GC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIXME:** Calculate $\\sigma_t$. Its uncertainty can be found by the rules of error propogation: if $f$ is a function of variables $g$ and $h$, then the errors $\\epsilon$ go as\n",
    "$$ \\epsilon_f^2 = \\epsilon_g^2 \\left(\\frac{\\partial f}{\\partial g}\\right)^2 + \\epsilon_h^2 \\left(\\frac{\\partial f}{\\partial h}\\right)^2 $$\n",
    "In this case that gives\n",
    "$$ \\epsilon_t = \\frac{\\sqrt{\\epsilon_{\\theta}^2 \\sigma_{\\theta}^2 + \\epsilon_{\\phi}^2 \\sigma_{\\phi}^2}}{\\sigma_t} $$\n",
    "\n",
    "(you should confirm this relation for yourself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_theta_obs = #FIXME\n",
    "sigma_theta_err_obs = #FIXME\n",
    "sigma_phi_obs = #FIXME\n",
    "sigma_phi_err_obs = #FIXME\n",
    "\n",
    "sigmat_obs = #FIXME\n",
    "sigmat_err_obs = np.sqrt(sigma_theta_obs**2 * sigma_theta_err_obs**2 + \\\n",
    "                        sigma_phi_obs**2 * sigma_phi_err_obs**2) / sigmat_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(midrad.value, sigmar_obs.value, sigmar_err_obs.value, label='$\\sigma_r$')\n",
    "plt.errorbar(midrad.value, sigmat_obs.value, sigmat_err_obs.value, label='$\\sigma_t$')\n",
    "plt.xlabel('R (kpc)')\n",
    "plt.ylabel('$\\sigma$ (km/s)')\n",
    "plt.xscale('log')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Use the Jeans equation\n",
    "\n",
    "In order to use the Jeans equation, we need $\\nu$, which we can get from the number of GCs per radial bin, along with the edges of the bin, since the volume of the bin is the spherical shell with those radii as boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_volume = 4./3. * np.pi * (binedges[1:]**3 - binedges[:-1])\n",
    "number_density = N_GC / shell_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot it\n",
    "plt.plot(midrad, number_density)\n",
    "plt.xlabel('R (kpc)')\n",
    "plt.ylabel('$\\\\nu$ (kpc$^{-3}$)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the Jeans equation, you will need to take a derivative of your data.\n",
    "You might be tempted to use ``np.gradient``. However, it turns out that, for the case of a variable that can change by many orders of magnitude, it always overestimates the magnitude of the slope, sometimes quite severely (and this is exacerbated if the function is sampled logarithmically instead of linearly, like in our case). However, it works well if the data is first transformed into log-log space before using ``np.gradient``, i.e. you can use the relation:\n",
    "\n",
    "$$ \\frac{dy}{dx} = \\frac{y}{x} \\frac{d \\ln y}{d \\ln x} $$\n",
    "\n",
    "This is implemented in the following function, which should also treat units correctly. If you have a variable ``y`` that is sampled at points ``x``, then you can calculate the derivative at those sampled points as $\\frac{dy}{dx} = $``power_gradient(y,x)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_gradient(y, x):\n",
    "    # Extract units if it has any\n",
    "    if hasattr(y, 'unit'):\n",
    "        y_unitless = y.value\n",
    "    else:\n",
    "        y_unitless = y\n",
    "    if hasattr(x, 'unit'):\n",
    "        x_unitless = x.value\n",
    "    else:\n",
    "        x_unitless = x\n",
    "        \n",
    "    powerlaw_slope = np.gradient(np.log(y_unitless), np.log(x_unitless))\n",
    "    return powerlaw_slope * y / x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIXME:** Calculate the enclosed mass $M(<r)$ using the Jeans equation, assuming isotropy. You might find it easiest to define this as a function that takes as parameters the radius coordinates, $\\sigma_r$ measured at those radii, $\\nu$ measured at those radii, and the anisotropy $\\beta$ (we will need this later, but you can give it a default value of 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Menclosed(radius, sigmar, nu, beta=0.0):\n",
    "    return #FIXME\n",
    "\n",
    "Menclosed_iso = Menclosed(midrad, sigmar_obs, number_density)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(midrad, Menclosed_iso.to(u.Msun).value)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('R (kpc)')\n",
    "plt.ylabel('$M(<r)$ ($M_{\\odot}$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at it! What do you think? Is there anything strange about it? Why?\n",
    "\n",
    "Let's compare it to the mass due to the baryonic parts of the Galaxy, i.e. the bulge and disk. We can use the [Bovy (2015)](http://arxiv.org/abs/1412.3451) fit, which is implemented in ``galpy.potential.MWPotential2014``, taking only the disk and bulge pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First print out the three components of the MWPotential2014 potential to make sure that you've got the right ones.\n",
    "# This should print out a list of three potentials and look something like:\n",
    "# [<galpy.potential.PowerSphericalPotentialwCutoff.PowerSphericalPotentialwCutoff at 0x181da64b00>,\n",
    "#  <galpy.potential.MiyamotoNagaiPotential.MiyamotoNagaiPotential at 0x181e250630>,\n",
    "#  <galpy.potential.TwoPowerSphericalPotential.NFWPotential at 0x181e250668>]\n",
    "\n",
    "# The ones you want are the PowerSphericalPotentialwCutoff (bulge) and MiyamotoNagaiPotential (disk), but NOT\n",
    "# the NFWPotential.\n",
    "# Python is zero-indexed, meaning that MWPotential2014[0] is the first potential in the list, MWPotential2014[1]\n",
    "# is the second potential in the list, and MWPotential2014[2] is the third potential in the list.\n",
    "\n",
    "potential.MWPotential2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIXME:** Create a potential with just the disk and bulge components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, if yours come up in the same order as above, then you would say:\n",
    "# MW_disk_bulge = potential.MWPotential2014[0] + potential.MWPotential2014[1]\n",
    "# But yours might be in a different order, so you need to do this yourself\n",
    "\n",
    "MW_disk_bulge = #FIXME\n",
    "\n",
    "# Make sure that it is set to output in physical units.\n",
    "potential.turn_physical_on(MW_disk_bulge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the enclosed mass from the Jeans equation to the mass from baryons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rax = np.logspace(np.log10(innermost_radius), np.log10(outermost_radius), 200)*u.kpc\n",
    "plt.plot(midrad, Menclosed_iso.to(u.Msun).value, label='Jeans Isotropic')\n",
    "plt.plot(rax, [potential.mass(MW_disk_bulge, r).to(u.Msun).value for r in rax], label='Bulge+Disk')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('R (kpc)')\n",
    "plt.ylabel('$M(<r)$ ($M_{\\odot}$)')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at it! What do you think? Are there places where the Jeans fit is higher? If so, what does that mean? Are there places where they are the same? If so, what does that mean? Are there places where the Jeans fit is lower? If so, what does that mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Anisotropy\n",
    "\n",
    "Let's see how anisotropy affects the results. Start off by seeing what happens if there is a constant radial bias, $\\beta=0.5$, or a constant tangential bias, $\\beta = -0.5$.\n",
    "\n",
    "**FIXME:** Use the Jeans equation to calculate the enclosed mass profile $M(<r)$ for each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Menclosed_radial = #FIXME\n",
    "Menclosed_tangential = #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(midrad, Menclosed_tangential.to(u.Msun).value, label='Tangentially-biased')\n",
    "plt.plot(midrad, Menclosed_iso.to(u.Msun).value, label='Isotropic')\n",
    "plt.plot(midrad, Menclosed_radial.to(u.Msun).value, label='Radially-biased')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('R (kpc)')\n",
    "plt.ylabel('$M(<r)$ ($M_{\\odot}$)')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at them! What do you think about the \"mass-anisotropy degeneracy\"? Why, physically, do the effects go in this direction?\n",
    "\n",
    "In the era of Gaia, we know the 3D velocities, so we have the tangential velocities and don't need to guess about the anisotropy -- we can measure $\\beta$ directly as a function of radius! Let's do that.\n",
    "\n",
    "**FIXME:** Calculate $\\beta$ from the data, i.e. from the variables ``sigmar_obs`` and ``sigmat_obs``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_true = #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(midrad, beta_true)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('R (kpc)')\n",
    "plt.ylabel('$\\\\beta$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Are globular cluster orbits more or less isotropic, mainly tangential, or mainly radial? How does the anisotropy depend on radius? Is it reasonable to use a constant value of $\\beta$ for convenience?\n",
    "\n",
    "Now we can use the Jeans equation using the actual $\\beta$ profile we found, and also using the average $\\beta$.\n",
    "\n",
    "**FIXME:** Calculate the enclosed mass for the actual $\\beta$ profile and the average $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_ave = np.mean(beta_true)\n",
    "print('Average beta:',beta_ave)\n",
    "Menclosed_beta_ave = #FIXME\n",
    "Menclosed_beta_true = #FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot them, again comparing to the Milky Way's bulge+disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rax = np.logspace(np.log10(innermost_radius), np.log10(outermost_radius), 200)*u.kpc\n",
    "plt.plot(midrad, Menclosed_beta_true.to(u.Msun).value, label='Jeans empirical $\\\\beta(r)$')\n",
    "plt.plot(midrad, Menclosed_beta_ave.to(u.Msun).value, label='Jeans average $\\\\beta$')\n",
    "plt.plot(midrad, Menclosed_iso.to(u.Msun).value, label='Jeans Isotropic')\n",
    "plt.plot(rax, [potential.mass(MW_disk_bulge, r).to(u.Msun).value for r in rax], label='Bulge+Disk')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('R (kpc)')\n",
    "plt.ylabel('$M(<r)$ ($M_{\\odot}$)')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at this! What do you think? Ask yourself the same questions as the first time we compared the Jeans analysis result to the baryonic component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Add an NFW halo\n",
    "\n",
    "Let's try to fit an NFW halo component to the enclosed mass profile and see what its parameters are. As in Part 3, we will use ``curve_fit`` which means we need to strip the units out of the actual fitting. We will use the unit system (kpc, $M_{\\odot}$).\n",
    "\n",
    "First we need to define the function that we will be fitting, which returns the enclosed mass for a combined potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MWdiskbulge_plus_NFW_potential(nfw_amp, nfw_rs):\n",
    "    halo_potential = potential.NFWPotential(amp=nfw_amp*u.Msun, a=nfw_rs*u.kpc)\n",
    "    total_potential = MW_disk_bulge + halo_potential\n",
    "    return total_potential\n",
    "\n",
    "# Return the enclosed mass of the disk + bulge + NFW halo potential, in units of Msun but with the units removed.\n",
    "# Note that the input parameters also need to be unitless -- we are going to use units of solar masses for\n",
    "# the NFW amplitude and kpc for the NFW scale radius.\n",
    "# Also make the input radius in kpc.\n",
    "def MWdiskbulge_plus_NFW_mass(radius, nfw_amp, nfw_rs):\n",
    "    total_potential = MWdiskbulge_plus_NFW_potential(nfw_amp, nfw_rs)\n",
    "    return [potential.mass(total_potential, r*u.kpc).to(u.Msun).value for r in radius]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the fit. Let's use the full $\\beta(r)$ from the data.\n",
    "\n",
    "**FIXME:** Give a reasonable initial guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess_nfw_amp = #FIXME\n",
    "initial_guess_nfw_rs = #FIXME\n",
    "initial_guess = [initial_guess_nfw_amp, initial_guess_nfw_rs]\n",
    "\n",
    "MWdiskbulge_plus_NFW_fit_parms, MWdiskbulge_plus_NFW_fit_pcov = curve_fit(MWdiskbulge_plus_NFW_mass, \\\n",
    "    midrad.value, Menclosed_beta_true.value, p0=initial_guess)\n",
    "\n",
    "MWdiskbulge_plus_NFW_fit_parms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at these values. What do you think of them?\n",
    "\n",
    "Let's plot the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rax = np.logspace(np.log10(innermost_radius), np.log10(outermost_radius), 200)*u.kpc\n",
    "plt.plot(midrad, Menclosed_beta_true.to(u.Msun).value, label='Jeans Eq')\n",
    "plt.plot(rax, MWdiskbulge_plus_NFW_mass(rax.value, *MWdiskbulge_plus_NFW_fit_parms), label='NFW+baryon fit')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('R (kpc)')\n",
    "plt.ylabel('$M(<r)$ ($M_{\\odot}$)')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at it! What do you think of the fit?\n",
    "\n",
    "It looks like we forgot the lesson from Part 2 about fitting in log space when we have values varying over many orders of magnitude. Let's do the fit in log space instead to see if that alters the conclusions. We'll just create a new wrapper function that returns the logarithm of the enclosed mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MWdiskbulge_plus_NFW_logmass(r, nfw_amp, nfw_rs):\n",
    "    return np.log(MWdiskbulge_plus_NFW_mass(r, nfw_amp, nfw_rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MWdiskbulge_plus_NFW_logfit_parms, MWdiskbulge_plus_NFW_logfit_pcov = curve_fit(MWdiskbulge_plus_NFW_logmass, \\\n",
    "    midrad.value, np.log(Menclosed_beta_true.value), p0=initial_guess)\n",
    "\n",
    "MWdiskbulge_plus_NFW_logfit_parms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at these values. What do you think of them?\n",
    "\n",
    "Let's plot both fits, both in linear and log space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "ax1.errorbar(midrad, Menclosed_beta_true.to(u.Msun).value, label='Jeans Eq')\n",
    "ax1.plot(rax, MWdiskbulge_plus_NFW_mass(rax.value, *MWdiskbulge_plus_NFW_fit_parms), label='Linear fit')\n",
    "ax1.plot(rax, MWdiskbulge_plus_NFW_mass(rax.value, *MWdiskbulge_plus_NFW_logfit_parms), label='Log Space fit')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('R (kpc)')\n",
    "ax1.set_ylabel('$M(<r)$ ($M_{\\odot}$)')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.errorbar(midrad, Menclosed_beta_true.to(u.Msun).value, label='Jeans Eq')\n",
    "ax2.plot(rax, MWdiskbulge_plus_NFW_mass(rax.value, *MWdiskbulge_plus_NFW_fit_parms), label='Linear fit')\n",
    "ax2.plot(rax, MWdiskbulge_plus_NFW_mass(rax.value, *MWdiskbulge_plus_NFW_logfit_parms), label='Log Space fit')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xlabel('R (kpc)')\n",
    "ax2.set_ylabel('$M(<r)$ ($M_{\\odot}$)')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at them. What do you think of the fits? How much of a difference does it make in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. How about if we fit the potential to the velocity dispersions?\n",
    "\n",
    "Now that we have an estimate of the halo properties, let's go back and see whether it does a good job of predicting the original velocity data.\n",
    "\n",
    "To do this, we need to use galpy's ``df.jeans.sigmar()`` function, which takes a potential, a (single, unfortunately) radius, and a function $\\nu(r)$ of the tracer population. So the first step is creating a function that will give $\\nu(r)$ for any $r$. Given how it looks, a power law (which is a line in log-log space) should do a good job. We will use ``scipy.stats.linregress`` to fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a line in log-log space, which is a power law\n",
    "nu_powerlaw_fit = linregress(np.log(midrad.value), np.log(number_density))\n",
    "# Define a function that implements this power law. Takes radius in kpc with units stripped.\n",
    "def nu_powerlaw(r):\n",
    "    return np.exp(nu_powerlaw_fit.intercept + nu_powerlaw_fit.slope*np.log(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot it to make sure it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(midrad, number_density, label='$\\\\nu$')\n",
    "plt.plot(rax, nu_powerlaw(rax.value), linestyle='dashed', label='Power law fit')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('R (kpc)')\n",
    "plt.ylabel('$\\nu$')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at it. Does it look okay?\n",
    "\n",
    "We'll need to do something similar for $\\beta$. In this case, it appears that we can approximate $\\beta$ as a linear function of $\\ln(r)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a line in log-linear space\n",
    "beta_regress_parms = linregress(np.log(midrad.value), beta_true)\n",
    "\n",
    "# Define a function that implements this fit. Takes radius in kpc with units stripped.\n",
    "def beta_interp(x):\n",
    "    return beta_regress_parms.intercept + beta_regress_parms.slope * np.log(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot it to make sure it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(midrad, beta_true, label='$\\\\beta$')\n",
    "plt.plot(rax, beta_interp(rax.value), linestyle='dashed', label='Linear fit')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('R (kpc)')\n",
    "plt.ylabel('$\\\\beta$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at it. Does it look okay?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's feed them into ``sigmar()``. We're going to evaluate it just at the radii where we have the measurements.\n",
    "\n",
    "**FIXME:** Pick whether to use the linear space fit or log space fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: Uncomment one of these depending on which fit you prefer.\n",
    "#bestfit_potential_parms = MWdiskbulge_plus_NFW_fit_parms\n",
    "#bestfit_potential_parms = MWdiskbulge_plus_NFW_logfit_parms\n",
    "\n",
    "bestfit_potential = MWdiskbulge_plus_NFW_potential(*bestfit_potential_parms)\n",
    "\n",
    "# The df.jeans module in galpy is apparently new and it simply does not work with physical units.\n",
    "# So we need to extract the physical/internal scaling units for the potential, run df.jeans.sigmar()\n",
    "# using internal units, and stick them on again at the end.\n",
    "# The radial unit in kpc is physical_conversion['ro'], and the velocity unit in km/s is physical_conversion['vo'].\n",
    "# We also need helper functions to take these weird-unit functions and translate them back to the\n",
    "# nu and beta fit functions.\n",
    "physical_conversion = galpy.util.conversion.get_physical(bestfit_potential)\n",
    "potential.turn_physical_off(bestfit_potential)\n",
    "nu_powerlaw_nophys = lambda x: nu_powerlaw(x*physical_conversion['ro'])\n",
    "beta_interp_nophys = lambda x: beta_interp(x*physical_conversion['ro'])\n",
    "\n",
    "# Calculate sigma_r\n",
    "sigmar_from_jeans = np.array([df.jeans.sigmar(bestfit_potential, x/physical_conversion['ro'], dens=nu_powerlaw_nophys,\\\n",
    "                                    beta=beta_interp_nophys) for x in midrad.value]) * physical_conversion['vo']\n",
    "\n",
    "# Turn physical units back on, in case we want them again later.\n",
    "potential.turn_physical_on(bestfit_potential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(midrad.value, sigmar_obs.value, yerr=sigmar_err_obs.value, label='Observed')\n",
    "plt.plot(midrad, sigmar_from_jeans, label='Model')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('R (kpc)')\n",
    "plt.ylabel('$\\sigma_r$')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at it. How well does it fit?\n",
    "\n",
    "So far we have started with the observational velocity data, turned it into an enclosed mass profile, fit the enclosed mass profile to a model, and then checked to see how well that matches the original velocity data. We could, instead, directly fit the model to the original velocity data. Let's try that!\n",
    "\n",
    "**FIXME:** Define a function that returns $\\sigma_r$ for a given set of NFW halo parameters. You can base your function on ``MWdiskbulge_plus_NFW_mass``, but using ``sigmar_from_jeans`` as calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MWdiskbulge_plus_NFW_sigmar(r, nfw_amp, nfw_rs):\n",
    "    # FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FIXME:** Use this function to fit the observational $\\sigma_r$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MWdiskbulge_plus_NFW_sigmarfit_parms, MWdiskbulge_plus_NFW_sigmarfit_pcov = curve_fit(#FIXME\n",
    "\n",
    "MWdiskbulge_plus_NFW_sigmarfit_parms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at these values. What do you think of them? How do they compare to the values from fitting the enclosed mass profile?\n",
    "\n",
    "Let's check both fits, both looking at the enclosed mass and $\\sigma_r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmar_from_jeans_Mencfit = MWdiskbulge_plus_NFW_sigmar(midrad.value, *bestfit_potential_parms)\n",
    "sigmar_from_jeans_sigmarfit = MWdiskbulge_plus_NFW_sigmar(midrad.value, *MWdiskbulge_plus_NFW_sigmarfit_parms)\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "ax1.errorbar(midrad.value, sigmar_obs.value, yerr=sigmar_err_obs.value, label='Observed')\n",
    "ax1.plot(midrad, sigmar_from_jeans_Mencfit, label='Fit to $M(<r)$')\n",
    "ax1.plot(midrad, sigmar_from_jeans_sigmarfit, label='Fit to $\\\\sigma_r$')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('R (kpc)')\n",
    "ax1.set_ylabel('$\\sigma_r$')\n",
    "ax1.legend(loc='best')\n",
    "\n",
    "ax2.errorbar(midrad.value, Menclosed_beta_true.to(u.Msun).value, label='Jeans Eq')\n",
    "ax2.plot(rax, MWdiskbulge_plus_NFW_mass(rax.value, *bestfit_potential_parms), label='Fit to $M(<r)$')\n",
    "ax2.plot(rax, MWdiskbulge_plus_NFW_mass(rax.value, *MWdiskbulge_plus_NFW_sigmarfit_parms), label='Fit to $\\\\sigma_r$')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xlabel('R (kpc)')\n",
    "ax2.set_ylabel('$M(<r)$ ($M_{\\odot}$)')\n",
    "ax2.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about these two fits?\n",
    "\n",
    "This is an example of the difference between a *parametric* approach and a *non-parametric* approach:\n",
    " - *Non-parametric*: Calculate derived quantities directly from the data without assuming any functional form. For example, ``Menclosed_beta_true``.\n",
    " - *Parametric*: Assume a functional form for the model and find the best fit parameters of the model given the data. For example, ``MWdiskbulge_plus_NFW_sigmarfit_parms`` and the corresponding enclosed mass profile.\n",
    " \n",
    "Using your experience as an example, what are some of the advantages and disadvantages of each approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Do something interesting\n",
    "\n",
    "**FIXME:** Try something interesting! A few ideas:\n",
    " - Propogate the uncertainties all the way through, including into the fitting. How much of a difference does it make?\n",
    " - Try different halo potentials for the fit. Does it affect your conclusions?\n",
    " - Find a different dataset of Milky Way halo objects that have distances and velocities (e.g. satellite galaxies, blue horizontal branch stars, the [Battaglia et al. (2005)](https://ui.adsabs.harvard.edu/abs/2005MNRAS.364..433B/abstract) dataset that we discussed in class) and see how much of a difference it makes.\n",
    " - Use a different model for the Milky Way bulge + disk. How much does it affect the halo parameters?\n",
    " - Use a different binning scheme (different number of bins, linearly-spaced bins, equal-number bins). What effect does it have?\n",
    " - See why I wrote the custom ``power_gradient`` derivative function -- what happens if you don't, and why does that happen numerically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
